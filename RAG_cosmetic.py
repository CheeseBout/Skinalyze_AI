
import os
import pandas as pd
from pathlib import Path
import chromadb
from langchain_community.document_loaders import TextLoader
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import torch
import time
from getpass import getpass
from PIL import Image
import google.generativeai as genai
from datetime import datetime
import json

# =============================================================================
# C·∫§U H√åNH - THAY ƒê·ªîI C√ÅC ƒê∆Ø·ªúNG D·∫™N N√ÄY
# =============================================================================
CHUNKS_FILE = Path(r"D:\rag-cosmetic-chatbot\data\product_chunks.txt")
PERSIST_DIRECTORY = Path(r"D:\rag-cosmetic-chatbot\db_chroma")
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
CHAT_HISTORY_DIR = Path(r"D:\rag-cosmetic-chatbot\chat-history")  # Th∆∞ m·ª•c l∆∞u l·ªãch s·ª≠ chat

# Global cache cho embeddings ƒë·ªÉ tr√°nh load l·∫°i
_CACHED_EMBEDDINGS = None

# =============================================================================
# THI·∫æT L·∫¨P API KEY
# =============================================================================
def setup_api_key():
    """Thi·∫øt l·∫≠p Google API Key"""
    if "GOOGLE_API_KEY" not in os.environ:
        print("\nüîë C·∫ßn Google API Key ƒë·ªÉ s·ª≠ d·ª•ng Gemini")
        print("üí° L·∫•y key mi·ªÖn ph√≠ t·∫°i: https://makersuite.google.com/app/apikey\n")
        api_key = "AIzaSyDLKLqpBHxf3xiutoYk5MjMzTywvju0Dx0"
        os.environ["GOOGLE_API_KEY"] = api_key
        print("‚úÖ ƒê√£ thi·∫øt l·∫≠p API Key!\n")
    else:
        print("‚úÖ API Key ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh s·∫µn!\n")
    
    # Configure genai for vision
    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])

# =============================================================================
# LOAD HO·∫∂C T·∫†O VECTOR STORE
# =============================================================================
def load_or_create_vectorstore():
    """Load vector store c√≥ s·∫µn ho·∫∑c t·∫°o m·ªõi n·∫øu ch∆∞a c√≥, v·ªõi error handling."""
    global _CACHED_EMBEDDINGS
    
    print("=" * 80)
    print("üìö KH·ªûI T·∫†O VECTOR STORE")
    print("=" * 80)
    
    db = None
    embeddings = None
    
    try: # <<< Try ch√≠nh bao quanh to√†n b·ªô h√†m >>>
        
        # ----- T·∫£i Embedding Model (v·ªõi cache) -----
        if _CACHED_EMBEDDINGS is not None:
            print(f"\n‚ö° S·ª≠ d·ª•ng cached embedding model")
            embeddings = _CACHED_EMBEDDINGS
        else:
            print(f"\n‚è≥ ƒêang t·∫£i embedding model: {MODEL_NAME}...")
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"   üñ•Ô∏è S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")
            
            try: # <<< Try cho vi·ªác t·∫£i embedding model >>>
                embeddings = HuggingFaceEmbeddings(
                    model_name=MODEL_NAME,
                    model_kwargs={'device': device},
                    encode_kwargs={'normalize_embeddings': True}
                )
                _CACHED_EMBEDDINGS = embeddings  # Cache l·∫°i
                print("‚úÖ ƒê√£ t·∫£i embedding model!\n")
            except Exception as e_embed_load:
                print(f"\n‚ùå L·ªñI NGHI√äM TR·ªåNG khi t·∫£i embedding model: {e_embed_load}")
                print("   Ki·ªÉm tra l·∫°i t√™n model, k·∫øt n·ªëi m·∫°ng v√† c√†i ƒë·∫∑t th∆∞ vi·ªán.")
                return None, None # Tr·∫£ v·ªÅ None n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c model

        # ----- Load ho·∫∑c T·∫°o Database -----
        if os.path.exists(PERSIST_DIRECTORY):
            print(f"üìÇ Ph√°t hi·ªán Vector Store c√≥ s·∫µn t·∫°i: {PERSIST_DIRECTORY}")
            print("‚è≥ ƒêang load database...\n")
            
            try: # <<< Try cho vi·ªác load DB c√≥ s·∫µn >>>
                db = Chroma(
                    persist_directory=str(PERSIST_DIRECTORY),
                    embedding_function=embeddings
                )
                
                # Ki·ªÉm tra xem collection c√≥ d·ªØ li·ªáu kh√¥ng
                count = db._collection.count() if db._collection else 0
                
                print(f"‚úÖ ƒê√£ load Vector Store th√†nh c√¥ng!")
                print(f"   üìä S·ªë documents trong database: {count}\n")
                if count == 0:
                     print("   ‚ö†Ô∏è C·∫£nh b√°o: Database c√≥ s·∫µn nh∆∞ng kh√¥ng c√≥ document n√†o.")

            except Exception as e_db_load:
                print(f"\n‚ùå L·ªñI khi load Vector Store c√≥ s·∫µn: {e_db_load}")
                print(f"   Th·ª≠ x√≥a th∆∞ m·ª•c '{PERSIST_DIRECTORY}' v√† ch·∫°y l·∫°i ƒë·ªÉ t·∫°o m·ªõi.")
                return None, embeddings # Tr·∫£ v·ªÅ embeddings ƒë√£ load ƒë∆∞·ª£c, nh∆∞ng db l√† None
                
        else:
            print(f"üÜï Kh√¥ng t√¨m th·∫•y Vector Store. ƒêang t·∫°o m·ªõi t·ª´ {CHUNKS_FILE.name}...\n")
            
            # --- C√°c b∆∞·ªõc t·∫°o DB m·ªõi ---
            docs = None
            try: # <<< Try cho vi·ªác load v√† split file chunks >>>
                # 1. Load file chunks
                print("üìñ [1/4] ƒêang load file chunks...")
                if not CHUNKS_FILE.exists():
                     raise FileNotFoundError(f"File chunk kh√¥ng t·ªìn t·∫°i t·∫°i: {CHUNKS_FILE}")
                loader = TextLoader(str(CHUNKS_FILE), encoding='utf-8')
                documents = loader.load()
                print(f"   ‚úì ƒê√£ load {len(documents)} document base")
                
                # 2. Split documents
                print("‚úÇÔ∏è  [2/4] ƒêang split th√†nh t·ª´ng chunk...")
                text_splitter = RecursiveCharacterTextSplitter(
                    separators=["---"], # T√°ch theo d·∫•u ---
                    chunk_size=400,   # Gi·∫£m chunk size ƒë·ªÉ LLM x·ª≠ l√Ω nhanh h∆°n
                    chunk_overlap=50,  # Th√™m overlap ƒë·ªÉ kh√¥ng m·∫•t context
                    length_function=len
                )
                docs = text_splitter.split_documents(documents)
                if not docs:
                     print("   ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng split ƒë∆∞·ª£c chunk n√†o. Ki·ªÉm tra file v√† separator.")
                     return None, embeddings # Kh√¥ng c√≥ docs ƒë·ªÉ t·∫°o DB
                print(f"   ‚úì ƒê√£ split th√†nh {len(docs)} chunks")
                
            except FileNotFoundError as e_file:
                 print(f"\n‚ùå L·ªñI: {e_file}")
                 return None, embeddings
            except Exception as e_load_split:
                 print(f"\n‚ùå L·ªñI khi load ho·∫∑c split file chunks: {e_load_split}")
                 return None, embeddings

            # --- T·∫°o embeddings v√† l∆∞u ---
            try: # <<< Try cho vi·ªác t·∫°o DB m·ªõi v√† th√™m docs >>>
                print("üíæ [3/4] ƒêang t·∫°o embeddings v√† l∆∞u v√†o database...")
                print("   (Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t, vui l√≤ng ƒë·ª£i...)\n")
                
                start_time = time.time()
                
                # X·ª≠ l√Ω theo batch
                batch_size = 50 # Gi·∫£m batch size n·∫øu g·∫∑p l·ªói b·ªô nh·ªõ
                total_docs = len(docs)
                
                if total_docs == 0:
                     print("   ‚ö†Ô∏è Kh√¥ng c√≥ chunk n√†o ƒë·ªÉ th√™m v√†o database.")
                     return None, embeddings # Kh√¥ng th·ªÉ t·∫°o DB r·ªóng theo c√°ch n√†y

                if total_docs <= batch_size:
                    # N·∫øu √≠t docs th√¨ t·∫°o m·ªôt l·∫ßn
                    print(f"   ‚è≥ ƒêang x·ª≠ l√Ω {total_docs} documents...")
                    db = Chroma.from_documents(
                        documents=docs,
                        embedding=embeddings,
                        persist_directory=str(PERSIST_DIRECTORY)
                    )
                else:
                    # N·∫øu nhi·ªÅu docs th√¨ chia batch
                    print(f"   ‚è≥ ƒêang x·ª≠ l√Ω theo batch ({batch_size} docs/batch)...")
                    
                    # Batch ƒë·∫ßu ti√™n - t·∫°o database
                    current_batch_docs = docs[:batch_size]
                    print(f"   ‚Üí Batch 1/{(total_docs-1)//batch_size + 1}: docs 0-{len(current_batch_docs)}")
                    db = Chroma.from_documents(
                        documents=current_batch_docs,
                        embedding=embeddings,
                        persist_directory=str(PERSIST_DIRECTORY)
                    )
                    
                    # C√°c batch ti·∫øp theo - th√™m v√†o database
                    for i in range(batch_size, total_docs, batch_size):
                        batch_start = i
                        batch_end = min(i + batch_size, total_docs)
                        current_batch_docs = docs[batch_start:batch_end]
                        batch_num = (i // batch_size) + 1
                        total_batches = (total_docs - 1) // batch_size + 1
                        
                        print(f"   ‚Üí Batch {batch_num}/{total_batches}: docs {batch_start}-{batch_end}")
                        if not current_batch_docs: # Ki·ªÉm tra batch r·ªóng (d∆∞ th·ª´a nh∆∞ng an to√†n)
                             continue
                        db.add_documents(current_batch_docs)
                        
                        # Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU n·∫øu d√πng CUDA
                        if device == 'cuda':
                            torch.cuda.empty_cache()
                
                end_time = time.time()
            
                print(f"\n   ‚úì Ho√†n th√†nh sau {end_time - start_time:.2f} gi√¢y")
                # Ki·ªÉm tra l·∫°i s·ªë l∆∞·ª£ng sau khi t·∫°o
                count_after_create = db._collection.count() if db and db._collection else 0
                print(f"   üìä ƒê√£ t·∫°o v√† l∆∞u {count_after_create} vectors")
                if count_after_create != total_docs:
                     print(f"   ‚ö†Ô∏è C·∫£nh b√°o: S·ªë vector l∆∞u ({count_after_create}) kh√¥ng kh·ªõp s·ªë chunk ({total_docs}).")

                print("\n‚úÖ ƒê√£ t·∫°o Vector Store th√†nh c√¥ng!")

            except Exception as e_db_create:
                 print(f"\n‚ùå L·ªñI NGHI√äM TR·ªåNG khi t·∫°o/l∆∞u Vector Store m·ªõi: {e_db_create}")
                 print(f"   Th·ª≠ ki·ªÉm tra dung l∆∞·ª£ng ·ªï ƒëƒ©a, quy·ªÅn ghi v√†o '{PERSIST_DIRECTORY}', ho·∫∑c gi·∫£m 'batch_size'.")
                 # X√≥a th∆∞ m·ª•c c√≥ th·ªÉ b·ªã t·∫°o d·ªü dang
                 if os.path.exists(PERSIST_DIRECTORY):
                      try:
                           import shutil
                           shutil.rmtree(PERSIST_DIRECTORY)
                           print(f"   ƒê√£ x√≥a th∆∞ m·ª•c '{PERSIST_DIRECTORY}' c√≥ th·ªÉ b·ªã l·ªói.")
                      except Exception as e_del:
                           print(f"   Kh√¥ng th·ªÉ x√≥a th∆∞ m·ª•c l·ªói '{PERSIST_DIRECTORY}': {e_del}")
                 db = None # ƒê·∫∑t l·∫°i db th√†nh None v√¨ t·∫°o l·ªói
                 return None, embeddings # Tr·∫£ v·ªÅ embeddings nh∆∞ng db l√† None

    except Exception as e_global: # <<< Except cho try ch√≠nh >>>
         print(f"\n‚ùå ƒê√É X·∫¢Y RA L·ªñI KH√îNG X√ÅC ƒê·ªäNH: {e_global}")
         return None, None # Tr·∫£ v·ªÅ None cho c·∫£ hai n·∫øu c√≥ l·ªói l·ªõn

    # N·∫øu m·ªçi th·ª© th√†nh c√¥ng
    return db, embeddings
# =============================================================================
# KH·ªûI T·∫†O RAG CHAIN
# =============================================================================
def setup_rag_chain(db):
    """Thi·∫øt l·∫≠p RAG chain v·ªõi Retriever, LLM v√† Prompt"""
    print("\n" + "=" * 80)
    print("‚õìÔ∏è KH·ªûI T·∫†O RAG CHAIN")
    print("=" * 80)
    
    # 1. Kh·ªüi t·∫°o LLM (t·ªëi ∆∞u cho t·ªëc ƒë·ªô)
    print("\nü§ñ [1/3] ƒêang k·∫øt n·ªëi v·ªõi Google Gemini...")
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",  # Gemini 2.0 stable - m·ªõi nh·∫•t, c√¢n b·∫±ng t·ªëc ƒë·ªô & quota
        temperature=0.1,  # Gi·∫£m temperature ƒë·ªÉ ph·∫£n h·ªìi nhanh h∆°n
        max_output_tokens=512,  # Gi·ªõi h·∫°n ƒë·ªô d√†i output ƒë·ªÉ nhanh h∆°n
        convert_system_message_to_human=True,
        request_options={"timeout": 60},  # Timeout 60s
        max_retries=2  # Ch·ªâ retry 2 l·∫ßn thay v√¨ m·∫∑c ƒë·ªãnh
    )
    print("   ‚úì ƒê√£ k·∫øt n·ªëi Gemini 2.0 Flash Stable (m·ªõi nh·∫•t, c√¢n b·∫±ng)")
    
    # 2. T·∫°o Retriever (gi·∫£m k ƒë·ªÉ nhanh h∆°n)
    print("üîç [2/3] ƒêang t·∫°o Retriever...")
    retriever = db.as_retriever(
        search_type="mmr",  # S·ª≠ d·ª•ng MMR ƒë·ªÉ gi·∫£m ƒë·ªô tr√πng l·∫∑p
        search_kwargs={
            "k": 2,  # Gi·∫£m xu·ªëng 2 chunks ƒë·ªÉ nhanh h∆°n
            "fetch_k": 5,  # Fetch 5 r·ªìi l·ªçc xu·ªëng 2
            "lambda_mult": 0.7  # C√¢n b·∫±ng gi·ªØa relevance v√† diversity
        }
    )
    print("   ‚úì Retriever s·∫Ω l·∫•y top 2 chunks ƒëa d·∫°ng nh·∫•t (MMR, si√™u nhanh)")
    
    # 3. T·∫°o Prompt Template (t·ªëi ∆∞u, ng·∫Øn g·ªçn h∆°n)
    print("üìù [3/3] ƒêang t·∫°o Prompt Template...")
    template = """B·∫°n l√† chuy√™n gia t∆∞ v·∫•n m·ªπ ph·∫©m chuy√™n nghi·ªáp, th√¢n thi·ªán v√† hi·ªÉu t√¢m l√Ω kh√°ch h√†ng.

PH√ÇN LO·∫†I C√ÇU H·ªéI V√Ä C√ÅCH TR·∫¢ L·ªúI:

üîπ **CH√ÄO H·ªéI/GIAO TI·∫æP C∆† B·∫¢N**
C√¢u h·ªèi: "xin ch√†o", "hi", "hello", "ch√†o b·∫°n", "hey"
‚Üí "Ch√†o b·∫°n! üëã M√¨nh l√† tr·ª£ l√Ω t∆∞ v·∫•n m·ªπ ph·∫©m. B·∫°n mu·ªën t√¨m s·∫£n ph·∫©m g√¨ h√¥m nay? üòä"

üîπ **H·ªéI V·ªÄ CH·ª®C NƒÇNG/GI·ªöI THI·ªÜU**
C√¢u h·ªèi: "b·∫°n l√† ai", "b·∫°n l√†m g√¨", "c√≥ th·ªÉ gi√∫p g√¨", "b·∫°n bi·∫øt g√¨"
‚Üí "M√¨nh l√† chuy√™n gia t∆∞ v·∫•n m·ªπ ph·∫©m! üíÑ M√¨nh c√≥ th·ªÉ gi√∫p b·∫°n:
‚Ä¢ T√¨m s·∫£n ph·∫©m theo lo·∫°i da (kh√¥, d·∫ßu, nh·∫°y c·∫£m, h·ªón h·ª£p, m·ª•n...)
‚Ä¢ T∆∞ v·∫•n kem d∆∞·ª°ng, serum, toner, m·∫∑t n·∫°, s·ªØa r·ª≠a m·∫∑t, kem ch·ªëng n·∫Øng
‚Ä¢ Gi·∫£i th√≠ch th√†nh ph·∫ßn v√† c√¥ng d·ª•ng s·∫£n ph·∫©m
‚Ä¢ G·ª£i √Ω routine chƒÉm s√≥c da
B·∫°n ƒëang g·∫∑p v·∫•n ƒë·ªÅ g√¨ v·ªÅ da ho·∫∑c c·∫ßn t√¨m s·∫£n ph·∫©m n√†o? üòä"

üîπ **H·ªéI V·ªÄ V·∫§N ƒê·ªÄ DA**
C√¢u h·ªèi: "da t√¥i b·ªã...", "t√¥i b·ªã m·ª•n", "da kh√¥", "da d·∫ßu", "da nh·∫°y c·∫£m"
‚Üí Ph√¢n t√≠ch v·∫•n ƒë·ªÅ v√† G·ª¢I √ù 1-2 s·∫£n ph·∫©m C·ª§ TH·ªÇ t·ª´ database ph√π h·ª£p nh·∫•t

üîπ **H·ªéI THEO LO·∫†I S·∫¢N PH·∫®M**
C√¢u h·ªèi: "c√≥ kem d∆∞·ª°ng n√†o...", "serum g√¨ t·ªët", "toner cho da...", "m·∫∑t n·∫°..."
‚Üí G·ª£i √Ω 1-2 s·∫£n ph·∫©m PH√ô H·ª¢P t·ª´ database, n√™u r√µ C√îNG D·ª§NG v√† LO·∫†I DA ph√π h·ª£p

üîπ **H·ªéI V·ªÄ TH∆Ø∆†NG HI·ªÜU**
C√¢u h·ªèi: "b·∫°n c√≥ [t√™n th∆∞∆°ng hi·ªáu] kh√¥ng", "s·∫£n ph·∫©m c·ªßa [brand]"
‚Üí Ki·ªÉm tra database, n·∫øu c√≥ th√¨ li·ªát k√™, n·∫øu kh√¥ng: "M√¨nh ch∆∞a c√≥ th√¥ng tin v·ªÅ [brand] trong database. B·∫°n mu·ªën t∆∞ v·∫•n s·∫£n ph·∫©m theo lo·∫°i da hay v·∫•n ƒë·ªÅ c·ª• th·ªÉ kh√¥ng? ÔøΩ"

üîπ **H·ªéI SO S√ÅNH**
C√¢u h·ªèi: "A hay B t·ªët h∆°n", "kh√°c nhau th·∫ø n√†o", "n√™n ch·ªçn c√°i n√†o"
‚Üí So s√°nh 2 s·∫£n ph·∫©m d·ª±a tr√™n TH√ÄNH PH·∫¶N, C√îNG D·ª§NG, LO·∫†I DA ph√π h·ª£p

üîπ **H·ªéI GI√Å/MUA ·ªû ƒê√ÇU**
C√¢u h·ªèi: "gi√° bao nhi√™u", "mua ·ªü ƒë√¢u", "c√≥ ship kh√¥ng"
‚Üí "Xin l·ªói, m√¨nh ch·ªâ t∆∞ v·∫•n v·ªÅ s·∫£n ph·∫©m th√¥i nh√©. B·∫°n c√≥ th·ªÉ mua t·∫°i c√°c store ch√≠nh h√£ng ho·∫∑c website c·ªßa th∆∞∆°ng hi·ªáu. M√¨nh c√≥ th·ªÉ t∆∞ v·∫•n th√™m v·ªÅ s·∫£n ph·∫©m kh√°c kh√¥ng? üòä"

üîπ **H·ªéI ROUTINE/C√ÅCH D√ôNG**
C√¢u h·ªèi: "routine cho da...", "th·ª© t·ª± d√πng", "d√πng nh∆∞ th·∫ø n√†o", "d√πng khi n√†o"
‚Üí G·ª£i √Ω routine c∆° b·∫£n: S√°ng (s·ªØa r·ª≠a m·∫∑t ‚Üí toner ‚Üí serum ‚Üí kem d∆∞·ª°ng ‚Üí ch·ªëng n·∫Øng), T·ªëi (t∆∞∆°ng t·ª± nh∆∞ng thay ch·ªëng n·∫Øng = m·∫∑t n·∫° 2-3 l·∫ßn/tu·∫ßn)

üîπ **C·∫¢M ∆†N/T·∫†M BI·ªÜT**
C√¢u h·ªèi: "c·∫£m ∆°n", "thank you", "ok r·ªìi", "t·∫°m bi·ªát", "bye"
‚Üí "Kh√¥ng c√≥ g√¨! üòä Ch√∫c b·∫°n c√≥ l√†n da ƒë·∫πp! H·∫πn g·∫∑p l·∫°i b·∫°n! üíï"

üîπ **C√ÇU H·ªéI NGO√ÄI L·ªÄ**
C√¢u h·ªèi: th·ªùi ti·∫øt, tin t·ª©c, th·ªÉ thao, ch√≠nh tr·ªã, to√°n h·ªçc...
‚Üí "Xin l·ªói, m√¨nh ch·ªâ chuy√™n v·ªÅ m·ªπ ph·∫©m v√† skincare th√¥i üíÑ B·∫°n c√≥ mu·ªën h·ªèi v·ªÅ chƒÉm s√≥c da kh√¥ng?"

---

**CH√ö √ù KHI TR·∫¢ L·ªúI:**
- Lu√¥n TH√ÇN THI·ªÜN, d√πng "m√¨nh/b·∫°n" thay v√¨ "t√¥i/b·∫°n" ƒë·ªÉ g·∫ßn g≈©i h∆°n
- N·∫øu t∆∞ v·∫•n s·∫£n ph·∫©m: T·ªêI ƒêA 2 s·∫£n ph·∫©m, n√™u r√µ T√äN - TH∆Ø∆†NG HI·ªÜU - C√îNG D·ª§NG - LO·∫†I DA
- D√πng emoji ph√π h·ª£p: üòäüíÑ‚ú®üíïüëã
- N·∫øu KH√îNG ch·∫Øc ch·∫Øn: "B·∫°n c√≥ th·ªÉ m√¥ t·∫£ c·ª• th·ªÉ h∆°n v·ªÅ [v·∫•n ƒë·ªÅ] ƒë·ªÉ m√¨nh t∆∞ v·∫•n ch√≠nh x√°c h∆°n kh√¥ng?"

TH√îNG TIN S·∫¢N PH·∫®M:
{context}

C√ÇU H·ªéI: {question}

TR·∫¢ L·ªúI (ng·∫Øn g·ªçn, 2-4 c√¢u):"""
    
    prompt = ChatPromptTemplate.from_template(template)
    print("   ‚úì ƒê√£ t·∫°o Prompt Template (compact + smart filtering)")
    
    # 4. X√¢y d·ª±ng RAG Chain
    def format_docs(docs):
        """Format documents th√†nh string (t·ªëi ∆∞u, lo·∫°i b·ªè th√¥ng tin d∆∞ th·ª´a)"""
        formatted = []
        for i, doc in enumerate(docs, 1):
            # Ch·ªâ l·∫•y th√¥ng tin quan tr·ªçng, b·ªè qua c√°c d√≤ng tr·ªëng
            content = doc.page_content.strip()
            if content:
                # Gi·ªõi h·∫°n ƒë·ªô d√†i m·ªói chunk ƒë·ªÉ gi·∫£m token
                if len(content) > 500:
                    content = content[:500] + "..."
                formatted.append(f"[{i}] {content}")
        return "\n".join(formatted)  # D√πng \n thay v√¨ \n\n ƒë·ªÉ compact h∆°n
    
    rag_chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    
    print("\n‚úÖ RAG Chain ƒë√£ s·∫µn s√†ng!")
    print("\nüìä Lu·ªìng ho·∫°t ƒë·ªông (SI√äU T·ªêI ∆ØU):")
    print("   1Ô∏è‚É£  User Question ‚Üí Retriever")
    print("   2Ô∏è‚É£  Retriever ‚Üí Top 2 chunks MMR (ƒëa d·∫°ng, kh√¥ng tr√πng l·∫∑p)")
    print("   3Ô∏è‚É£  Format chunks ‚Üí Context string (max 500 chars/chunk)")
    print("   4Ô∏è‚É£  Context + Question ‚Üí Prompt (compact)")
    print("   5Ô∏è‚É£  Prompt ‚Üí LLM (Gemini 2.0 Flash Exp, max_tokens=512)")
    print("   6Ô∏è‚É£  LLM ‚Üí Final Answer ‚ö°‚ö°‚ö°")

    return rag_chain

# =============================================================================
# CHAT HISTORY - QU·∫¢N L√ù L·ªäCH S·ª¨ H·ªòI THO·∫†I
# =============================================================================
def save_chat_history(chat_history):
    """L∆∞u l·ªãch s·ª≠ chat v√†o file JSON"""
    try:
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        CHAT_HISTORY_DIR.mkdir(exist_ok=True)
        
        # T√™n file theo timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = CHAT_HISTORY_DIR / f"chat_{timestamp}.json"
        
        # L∆∞u v√†o file
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(chat_history, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ ƒê√£ l∆∞u l·ªãch s·ª≠ chat: {filename.name}")
        return filename
    except Exception as e:
        print(f"\n‚ö†Ô∏è  L·ªói khi l∆∞u l·ªãch s·ª≠: {str(e)}")
        return None

def load_latest_chat_history():
    """Load l·ªãch s·ª≠ chat g·∫ßn nh·∫•t (n·∫øu c√≥)"""
    try:
        if not CHAT_HISTORY_DIR.exists():
            return None
        
        # T√¨m file m·ªõi nh·∫•t
        chat_files = list(CHAT_HISTORY_DIR.glob("chat_*.json"))
        if not chat_files:
            return None
        
        latest_file = max(chat_files, key=lambda f: f.stat().st_mtime)
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            history = json.load(f)
        
        return history, latest_file
    except Exception as e:
        print(f"\n‚ö†Ô∏è  L·ªói khi load l·ªãch s·ª≠: {str(e)}")
        return None

# =============================================================================
# VISION ANALYSIS - PH√ÇN T√çCH ·∫¢NH DA ƒê·ªÇ B·ªî SUNG TH√îNG TIN CHO RAG
# =============================================================================
def analyze_skin_image(image_path):
    """Ph√¢n t√≠ch ·∫£nh da b·∫±ng VLM - T·∫≠p trung v√†o m·ª©c ƒë·ªô nghi√™m tr·ªçng l√†m ƒë·∫ßu v√†o cho RAG"""
    try:
        print("\nüì∏ ƒêang ph√¢n t√≠ch t√¨nh tr·∫°ng da t·ª´ ·∫£nh...")
        
        # Load image
        img = Image.open(image_path)
        
        # Kh·ªüi t·∫°o Gemini Vision model
        vision_model = genai.GenerativeModel('gemini-2.5-flash')
        
        # Prompt t·∫≠p trung v√†o m·ª©c ƒë·ªô nghi√™m tr·ªçng
        vision_prompt = """B·∫°n l√† chuy√™n gia da li·ªÖu. Ph√¢n t√≠ch ·∫£nh da v√† T√ìM T·∫ÆT NG·∫ÆN G·ªåN:

1. LO·∫†I DA: (kh√¥/d·∫ßu/h·ªón h·ª£p/nh·∫°y c·∫£m/th∆∞·ªùng)

2. V·∫§N ƒê·ªÄ CH√çNH & M·ª®C ƒê·ªò NGHI√äM TR·ªåNG:
- N·∫øu c√≥ m·ª•n: lo·∫°i m·ª•n (vi√™m/ƒë·∫ßu ƒëen/ƒë·∫ßu tr·∫Øng/b·ªçc), m·ª©c ƒë·ªô (NH·∫∏/TRUNG B√åNH/N·∫∂NG/R·∫§T N·∫∂NG)
- N·∫øu c√≥ th√¢m/s·∫πo: m·ª©c ƒë·ªô (NH·∫∏/TRUNG B√åNH/N·∫∂NG/R·∫§T N·∫∂NG), m√†u s·∫Øc, ph√¢n b·ªë
- N·∫øu c√≥ l√£o h√≥a: m·ª©c ƒë·ªô (NH·∫∏/TRUNG B√åNH/N·∫∂NG)
- N·∫øu c√≥ v·∫•n ƒë·ªÅ kh√°c: n√™u r√µ

3. M·ª®C ƒê·ªò CHUNG: Ch·ªçn 1 trong 4:
   - NH·∫∏: V·∫•n ƒë·ªÅ nh·ªè, √≠t n·ªët, c√≥ th·ªÉ t·ª± chƒÉm s√≥c
   - TRUNG B√åNH: V·∫•n ƒë·ªÅ r√µ r√†ng, nhi·ªÅu n·ªët, c·∫ßn s·∫£n ph·∫©m chuy√™n d·ª•ng
   - N·∫∂NG: V·∫•n ƒë·ªÅ lan r·ªông, vi√™m nhi·ªÅu, c·∫ßn ƒëi·ªÅu tr·ªã t√≠ch c·ª±c
   - R·∫§T N·∫∂NG: Vi√™m tr·∫ßm tr·ªçng, s·∫πo nhi·ªÅu, c·∫ßn g·∫∑p b√°c sƒ© da li·ªÖu

4. G·ª¢I √ù: (1 c√¢u ng·∫Øn)

QUAN TR·ªåNG: Ph·∫£i ghi r√µ M·ª®C ƒê·ªò (NH·∫∏/TRUNG B√åNH/N·∫∂NG/R·∫§T N·∫∂NG).

Tr·∫£ l·ªùi NG·∫ÆN G·ªåN, b·∫±ng ti·∫øng Vi·ªát."""
        
        # G·ªçi vision model
        response = vision_model.generate_content([vision_prompt, img])
        analysis = response.text
        
        print("‚úÖ ƒê√£ ph√¢n t√≠ch xong!")
        
        return analysis
        
    except FileNotFoundError:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file ·∫£nh: {image_path}")
        return None
    except Exception as e:
        print(f"‚ùå L·ªói khi ph√¢n t√≠ch ·∫£nh: {str(e)}")
        return None

# =============================================================================
# INTERACTIVE CHAT
# =============================================================================
def chat(rag_chain):
    """Interactive chat trong terminal v·ªõi h·ªó tr·ª£ ph√¢n t√≠ch ·∫£nh da v√† l∆∞u l·ªãch s·ª≠"""
    print("\n" + "=" * 80)
    print("üí¨ COSMETIC CONSULTANT CHATBOT (‚ö° RAG + üì∏ VLM + üíæ HISTORY)")
    print("=" * 80)
    
    # Load l·ªãch s·ª≠ chat tr∆∞·ªõc ƒë√≥ (n·∫øu c√≥)
    previous_history = load_latest_chat_history()
    if previous_history:
        history, history_file = previous_history
        print(f"\nüìñ T√¨m th·∫•y l·ªãch s·ª≠ chat tr∆∞·ªõc: {history_file.name}")
        print(f"   S·ªë l∆∞·ª£ng: {len(history)} tin nh·∫Øn")
        view = input("   Xem l·ªãch s·ª≠? (y/n): ").strip().lower()
        if view == 'y':
            print("\n" + "=" * 80)
            print("ÔøΩ L·ªäCH S·ª¨ CHAT TR∆Ø·ªöC:")
            print("=" * 80)
            for msg in history[-10:]:  # Hi·ªÉn th·ªã 10 tin nh·∫Øn cu·ªëi
                role = "üßë B·∫°n" if msg['role'] == 'user' else "ü§ñ Bot"
                content = msg['content'][:200] + "..." if len(msg['content']) > 200 else msg['content']
                print(f"{role}: {content}")
                print("-" * 40)
            print("=" * 80)
    
    print("\nÔøΩüìù G√µ c√¢u h·ªèi c·ªßa b·∫°n v√† nh·∫•n Enter")
    print("üí° V√≠ d·ª• text: 'T√¥i c·∫ßn kem d∆∞·ª°ng cho da kh√¥ nh·∫°y c·∫£m'")
    print("üì∏ Ph√¢n t√≠ch ·∫£nh DA: G·ª≠i ƒë∆∞·ªùng d·∫´n ·∫£nh da c·ªßa b·∫°n (t·ª± ƒë·ªông nh·∫≠n di·ªán)")
    print("   ‚Üí VLM ph√¢n t√≠ch chi ti·∫øt t√¨nh tr·∫°ng da")
    print("   ‚Üí RAG t∆∞ v·∫•n s·∫£n ph·∫©m ph√π h·ª£p d·ª±a tr√™n ph√¢n t√≠ch")
    print("   V√≠ d·ª•: C:\\Users\\Photos\\my_skin.jpg")
    print("üö™ G√µ 'exit', 'quit' ho·∫∑c 'tho√°t' ƒë·ªÉ k·∫øt th√∫c v√† L∆ØU L·ªäCH S·ª¨")
    print("‚ö° C√¥ng ngh·ªá: VLM (Gemini 2.5 Flash) + RAG (ChromaDB)\n")
    print("=" * 80)
    
    # Kh·ªüi t·∫°o l·ªãch s·ª≠ chat m·ªõi v√† conversation memory
    chat_history = {
        'session_start': datetime.now().isoformat(),
        'messages': []
    }
    
    # Conversation memory - l∆∞u context trong phi√™n (bot s·∫Ω nh·ªõ!)
    conversation_context = []  # L∆∞u t·∫•t c·∫£ trao ƒë·ªïi: [(user_msg, bot_response), ...]
    
    # C√°c ƒëu√¥i file ·∫£nh ƒë∆∞·ª£c h·ªó tr·ª£
    IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.webp', '.bmp', '.gif', '.tiff')
    
    while True:
        print()
        try:
            # Nh·∫≠n input t·ª´ user
            question = input("üßë B·∫°n: ").strip()
            
            # Ki·ªÉm tra ƒëi·ªÅu ki·ªán tho√°t
            if not question:
                print("‚ö†Ô∏è  Vui l√≤ng nh·∫≠p c√¢u h·ªèi!")
                continue
                
            if question.lower() in ['exit', 'quit', 'tho√°t', 'bye', 'goodbye']:
                print("\nüëã C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng d·ªãch v·ª•!")
                # L∆∞u l·ªãch s·ª≠ tr∆∞·ªõc khi tho√°t
                if chat_history['messages']:
                    chat_history['session_end'] = datetime.now().isoformat()
                    save_chat_history(chat_history)
                print("=" * 80)
                break
            
            # T·ª± ƒë·ªông nh·∫≠n di·ªán ƒë∆∞·ªùng d·∫´n ·∫£nh
            # Lo·∫°i b·ªè d·∫•u ngo·∫∑c k√©p n·∫øu c√≥
            question_clean = question.strip('"').strip("'")
            
            # T√°ch ƒë∆∞·ªùng d·∫´n ·∫£nh v√† text (n·∫øu user g·ª≠i c·∫£ hai)
            image_path_candidate = None
            text_question = None
            
            # Ki·ªÉm tra n·∫øu c√≥ ƒë∆∞·ªùng d·∫´n file trong c√¢u h·ªèi
            # T√¨m ƒë∆∞·ªùng d·∫´n trong d·∫•u ngo·∫∑c k√©p tr∆∞·ªõc
            if '"' in question:
                # Extract path trong d·∫•u ngo·∫∑c k√©p
                import re
                matches = re.findall(r'"([^"]+)"', question)
                for match in matches:
                    if any(match.lower().endswith(ext) for ext in IMAGE_EXTENSIONS):
                        image_path_candidate = match
                        # Ph·∫ßn c√≤n l·∫°i l√† text question
                        text_question = question.replace(f'"{match}"', '').strip()
                        break
            
            # N·∫øu kh√¥ng c√≥ d·∫•u ngo·∫∑c, ki·ªÉm tra ƒë∆∞·ªùng d·∫´n tr·ª±c ti·∫øp
            if not image_path_candidate:
                # T√°ch c√¢u theo space ƒë·ªÉ t√¨m ƒë∆∞·ªùng d·∫´n
                words = question_clean.split()
                for word in words:
                    if any(word.lower().endswith(ext) for ext in IMAGE_EXTENSIONS):
                        # Ki·ªÉm tra n·∫øu l√† ƒë∆∞·ªùng d·∫´n h·ª£p l·ªá (c√≥ \ ho·∫∑c / ho·∫∑c :)
                        if '\\' in word or '/' in word or ':' in word:
                            image_path_candidate = word
                            # Ph·∫ßn c√≤n l·∫°i l√† text
                            text_question = question_clean.replace(word, '').strip()
                            break
            
            # N·∫øu to√†n b·ªô input l√† ƒë∆∞·ªùng d·∫´n ·∫£nh
            if not image_path_candidate and any(question_clean.lower().endswith(ext) for ext in IMAGE_EXTENSIONS):
                image_path_candidate = question_clean
            
            # X·ª≠ l√Ω n·∫øu c√≥ prefix image:/·∫£nh:/anh:
            if not image_path_candidate and question.lower().startswith(('image:', '·∫£nh:', 'anh:')):
                parts = question.split(':', 1)
                if len(parts) > 1:
                    image_path_candidate = parts[1].strip().strip('"').strip("'")
            
            # N·∫øu t√¨m th·∫•y ƒë∆∞·ªùng d·∫´n ·∫£nh
            if image_path_candidate:
                image_path = image_path_candidate
                
            # N·∫øu t√¨m th·∫•y ƒë∆∞·ªùng d·∫´n ·∫£nh
            if image_path_candidate:
                image_path = image_path_candidate
                
                # X·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi
                if not os.path.isabs(image_path):
                    image_path = os.path.join(os.getcwd(), image_path)
                
                # Ki·ªÉm tra file t·ªìn t·∫°i
                if not os.path.exists(image_path):
                    print(f"‚ùå Kh√¥ng t√¨m th·∫•y file ·∫£nh: {image_path}")
                    print("üí° Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n!")
                    print("-" * 80)
                    continue
                
                # B∆∞·ªõc 1: VLM ph√¢n t√≠ch ·∫£nh da
                skin_analysis = analyze_skin_image(image_path)
                
                # L∆∞u input user (·∫£nh)
                chat_history['messages'].append({
                    'timestamp': datetime.now().isoformat(),
                    'role': 'user',
                    'type': 'image',
                    'content': f"[G·ª≠i ·∫£nh: {os.path.basename(image_path)}]",
                    'image_path': image_path,
                    'additional_text': text_question if text_question else None
                })
                
                if skin_analysis:
                    # Ki·ªÉm tra m·ª©c ƒë·ªô nghi√™m tr·ªçng
                    analysis_upper = skin_analysis.upper()
                    is_very_severe = 'R·∫§T N·∫∂NG' in analysis_upper or 'R·∫§T NGHI√äM TR·ªåNG' in analysis_upper
                    
                    # Hi·ªÉn th·ªã c·∫£nh b√°o n·∫øu r·∫•t n·∫∑ng
                    if is_very_severe:
                        print("\n" + "‚ö†Ô∏è " * 20)
                        print("‚ö†Ô∏è  C·∫¢NH B√ÅO: T√åNH TR·∫†NG DA R·∫§T NGHI√äM TR·ªåNG!")
                        print("‚ö†Ô∏è " * 20)
                        print("\nüè• KHUY·∫æN C√ÅO:")
                        print("   ‚Ä¢ T√¨nh tr·∫°ng da c·ªßa b·∫°n C·∫¶N ƒë∆∞·ª£c b√°c sƒ© da li·ªÖu thƒÉm kh√°m")
                        print("   ‚Ä¢ Kh√¥ng n√™n t·ª± ƒëi·ªÅu tr·ªã ho·∫∑c ch·ªâ d√πng m·ªπ ph·∫©m")
                        print("   ‚Ä¢ Vui l√≤ng ƒë·∫∑t l·ªãch g·∫∑p b√°c sƒ© da li·ªÖu NGAY")
                        print("\n" + "=" * 80)
                        
                        # V·∫´n t∆∞ v·∫•n s·∫£n ph·∫©m h·ªó tr·ª£ nh∆∞ng c√≥ disclaimer
                        print("\nüí° Tuy nhi√™n, d∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë s·∫£n ph·∫©m H·ªñ TR·ª¢ (KH√îNG THAY TH·∫æ ƒëi·ªÅu tr·ªã y khoa):\n")
                    
                    # B∆∞·ªõc 2: K·∫øt h·ª£p ph√¢n t√≠ch VLM v·ªõi c√¢u h·ªèi ƒë·ªÉ query RAG
                    if text_question:
                        if is_very_severe:
                            rag_query = f"""T√¨nh tr·∫°ng da (R·∫§T NGHI√äM TR·ªåNG - C·∫¶N G·∫∂P B√ÅC Sƒ®):
{skin_analysis}

Y√™u c·∫ßu: {text_question}

G·ª£i √Ω 1-2 s·∫£n ph·∫©m H·ªñ TR·ª¢ NH·∫∏ NH√ÄNG (kh√¥ng thay th·∫ø ƒëi·ªÅu tr·ªã y khoa). 
NH·∫§N M·∫†NH: C·∫ßn g·∫∑p b√°c sƒ© da li·ªÖu."""
                        else:
                            rag_query = f"""T√¨nh tr·∫°ng da (t·ª´ ph√¢n t√≠ch ·∫£nh):
{skin_analysis}

Y√™u c·∫ßu: {text_question}

T∆∞ v·∫•n 2-3 s·∫£n ph·∫©m C·ª§ TH·ªÇ ph√π h·ª£p v·ªõi M·ª®C ƒê·ªò."""
                    else:
                        # Kh√¥ng c√≥ c√¢u h·ªèi, ch·ªâ d·ª±a v√†o ph√¢n t√≠ch
                        if is_very_severe:
                            rag_query = f"""T√¨nh tr·∫°ng da (R·∫§T NGHI√äM TR·ªåNG - C·∫¶N G·∫∂P B√ÅC Sƒ®):
{skin_analysis}

G·ª£i √Ω 1-2 s·∫£n ph·∫©m H·ªñ TR·ª¢ NH·∫∏ NH√ÄNG (kh√¥ng thay th·∫ø ƒëi·ªÅu tr·ªã y khoa).
NH·∫§N M·∫†NH: C·∫ßn g·∫∑p b√°c sƒ© da li·ªÖu."""
                        else:
                            rag_query = f"""T√¨nh tr·∫°ng da (t·ª´ ph√¢n t√≠ch ·∫£nh):
{skin_analysis}

T∆∞ v·∫•n 2-3 s·∫£n ph·∫©m C·ª§ TH·ªÇ ph√π h·ª£p v·ªõi M·ª®C ƒê·ªò."""
                    
                    print("\nüîé T√¨m s·∫£n ph·∫©m d·ª±a tr√™n m·ª©c ƒë·ªô nghi√™m tr·ªçng...")
                    time.sleep(1)
                    
                    product_recommendation = rag_chain.invoke(rag_query)
                    
                    # L∆∞u v√†o conversation context
                    user_input_desc = f"[G·ª≠i ·∫£nh da] {text_question if text_question else 'Ph√¢n t√≠ch v√† t∆∞ v·∫•n'}"
                    conversation_context.append((user_input_desc, product_recommendation))
                    
                    print("\nüíÑ T∆Ø V·∫§N S·∫¢N PH·∫®M:")
                    print("=" * 80)
                    print(product_recommendation)
                    print("=" * 80)
                    
                    # L∆∞u response c·ªßa bot
                    bot_response = product_recommendation
                    if is_very_severe:
                        bot_response = f"‚ö†Ô∏è C·∫¢NH B√ÅO: R·∫§T NGHI√äM TR·ªåNG - C·∫¶N G·∫∂P B√ÅC Sƒ®!\n\n{product_recommendation}"
                    
                    chat_history['messages'].append({
                        'timestamp': datetime.now().isoformat(),
                        'role': 'assistant',
                        'type': 'product_recommendation',
                        'content': bot_response,
                        'skin_analysis': skin_analysis,
                        'severity': 'VERY_SEVERE' if is_very_severe else 'NORMAL'
                    })
                    
                    # Nh·∫Øc l·∫°i c·∫£nh b√°o n·∫øu r·∫•t n·∫∑ng
                    if is_very_severe:
                        print("\n" + "‚ö†Ô∏è " * 20)
                        print("‚ö†Ô∏è  L∆ØU √ù: C√°c s·∫£n ph·∫©m tr√™n CH·ªà H·ªñ TR·ª¢, KH√îNG THAY TH·∫æ ƒëi·ªÅu tr·ªã y khoa!")
                        print("‚ö†Ô∏è  VUI L√íNG ƒê·∫∂T L·ªäCH G·∫∂P B√ÅC Sƒ® DA LI·ªÑU NGAY! üè•")
                        print("‚ö†Ô∏è " * 20)
                
                print("-" * 80)
                continue
            
            # X·ª≠ l√Ω c√¢u h·ªèi text th√¥ng th∆∞·ªùng
            print("\n‚è≥ ƒêang t√¨m ki·∫øm v√† t·∫°o c√¢u tr·∫£ l·ªùi...")
            start_time = time.time()
            
            # L∆∞u c√¢u h·ªèi user
            chat_history['messages'].append({
                'timestamp': datetime.now().isoformat(),
                'role': 'user',
                'type': 'text',
                'content': question
            })
            
            # Th√™m delay nh·ªè ƒë·ªÉ tr√°nh rate limit
            time.sleep(1)  # Ch·ªù 1 gi√¢y tr∆∞·ªõc m·ªói request
            
            # T·∫°o query v·ªõi context t·ª´ conversation history
            if conversation_context:
                # L·∫•y 3 c·∫∑p h·ªôi tho·∫°i g·∫ßn nh·∫•t ƒë·ªÉ l√†m context
                recent_context = conversation_context[-3:]
                context_str = "\n".join([
                    f"User ƒë√£ h·ªèi: {ctx[0]}\nBot ƒë√£ tr·∫£ l·ªùi: {ctx[1][:200]}..." 
                    for ctx in recent_context
                ])
                
                query_with_context = f"""L·ªäCH S·ª¨ H·ªòI THO·∫†I G·∫¶N ƒê√ÇY:
{context_str}

C√ÇU H·ªéI HI·ªÜN T·∫†I: {question}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n L·ªäCH S·ª¨ v√† c√¢u h·ªèi hi·ªán t·∫°i (n·∫øu user ƒëang h·ªèi ti·∫øp v·ªÅ c√πng topic)."""
                response = rag_chain.invoke(query_with_context)
            else:
                # L·∫ßn ƒë·∫ßu ti√™n, kh√¥ng c√≥ context
                response = rag_chain.invoke(question)
            
            elapsed_time = time.time() - start_time
            
            # L∆∞u v√†o conversation context (bot s·∫Ω nh·ªõ!)
            conversation_context.append((question, response))
            
            # In response
            print(f"\nü§ñ Bot: {response}")
            print(f"\n‚ö° Th·ªùi gian ph·∫£n h·ªìi: {elapsed_time:.2f}s")
            print("-" * 80)
            
            # L∆∞u response c·ªßa bot
            chat_history['messages'].append({
                'timestamp': datetime.now().isoformat(),
                'role': 'assistant',
                'type': 'text',
                'content': response,
                'response_time': elapsed_time
            })
            
        except KeyboardInterrupt:
            print("\n\nüëã ƒê√£ nh·∫≠n t√≠n hi·ªáu tho√°t. C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng!")
            print("=" * 80)
            break
            
        except Exception as e:
            print(f"\n‚ùå ƒê√£ c√≥ l·ªói x·∫£y ra: {str(e)}")
            print("üí° Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi kh√°c!")
            print("-" * 80)

# =============================================================================
# MAIN FUNCTION
# =============================================================================
def main():
    """Main function - ƒëi·ªÉm kh·ªüi ƒë·∫ßu ch∆∞∆°ng tr√¨nh"""
    try:
        print("\nüéØ COSMETIC RAG CHATBOT - INTERACTIVE MODE")
        print("=" * 80)
        
        # 1. Setup API Key
        setup_api_key()
        
        # 2. Load/Create Vector Store
        db, embeddings = load_or_create_vectorstore()
        
        # 3. Setup RAG Chain
        rag_chain = setup_rag_chain(db)
        
        # 4. Start Chat
        chat(rag_chain)
        
    except Exception as e:
        print(f"\n‚ùå L·ªñI NGHI√äM TR·ªåNG: {str(e)}")
        print("üí° Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh v√† th·ª≠ l·∫°i!")
        return 1
    
    return 0

# =============================================================================
# ENTRY POINT
# =============================================================================
if __name__ == "__main__":
    exit(main())