"""
RAG Cosmetic Chatbot Core - Stateless for NestJS Backend Integration
No local file storage, no session management - all handled by NestJS
"""

import os
from pathlib import Path
import torch
from PIL import Image
import google.generativeai as genai
import base64
import io

from langchain_community.document_loaders import TextLoader
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# =============================================================================
# CONFIGURATION
# =============================================================================
PATH = Path(__file__).parent.resolve()
CHUNKS_FILE = PATH / "data" / "product_chunks.txt"
PERSIST_DIRECTORY = PATH / "db_chroma"
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# Global cache for embeddings
_CACHED_EMBEDDINGS = None

# =============================================================================
# API KEY SETUP
# =============================================================================
def setup_api_key():
    """Setup Google API Key"""
    if "GOOGLE_API_KEY" not in os.environ:
        print("\nðŸ”‘ Cáº§n Google API Key Ä‘á»ƒ sá»­ dá»¥ng Gemini")
        print("ðŸ’¡ Láº¥y key miá»…n phÃ­ táº¡i: https://makersuite.google.com/app/apikey\n")
        api_key = "AIzaSyDLKLqpBHxf3xiutoYk5MjMzTywvju0Dx0"
        os.environ["GOOGLE_API_KEY"] = api_key
        print("âœ… ÄÃ£ thiáº¿t láº­p API Key!\n")
    else:
        print("âœ… API Key Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh sáºµn!\n")
    
    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])

# =============================================================================
# VECTOR STORE
# =============================================================================
def load_or_create_vectorstore():
    """Load or create vector store"""
    global _CACHED_EMBEDDINGS
    
    print("=" * 80)
    print("ðŸ“š KHá»žI Táº O VECTOR STORE")
    print("=" * 80)
    
    try:
        # Load embedding model
        if _CACHED_EMBEDDINGS is not None:
            print(f"\nâš¡ Sá»­ dá»¥ng cached embedding model")
            embeddings = _CACHED_EMBEDDINGS
        else:
            print(f"\nâ³ Äang táº£i embedding model: {MODEL_NAME}...")
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"   ðŸ–¥ï¸ Sá»­ dá»¥ng thiáº¿t bá»‹: {device}")
            
            embeddings = HuggingFaceEmbeddings(
                model_name=MODEL_NAME,
                model_kwargs={'device': device},
                encode_kwargs={'normalize_embeddings': True}
            )
            _CACHED_EMBEDDINGS = embeddings
            print("âœ… ÄÃ£ táº£i embedding model!\n")

        # Load or create database
        if os.path.exists(PERSIST_DIRECTORY):
            print(f"ðŸ“‚ Loading Vector Store from: {PERSIST_DIRECTORY}")
            db = Chroma(
                persist_directory=str(PERSIST_DIRECTORY),
                embedding_function=embeddings
            )
            count = db._collection.count() if db._collection else 0
            print(f"âœ… Loaded {count} documents\n")
        else:
            print(f"ðŸ†• Creating new Vector Store from {CHUNKS_FILE.name}...\n")
            
            if not CHUNKS_FILE.exists():
                raise FileNotFoundError(f"File khÃ´ng tá»“n táº¡i: {CHUNKS_FILE}")
            
            loader = TextLoader(str(CHUNKS_FILE), encoding='utf-8')
            documents = loader.load()
            print(f"   âœ“ Loaded {len(documents)} document base")
            
            text_splitter = RecursiveCharacterTextSplitter(
                separators=["---"],
                chunk_size=400,
                chunk_overlap=50,
                length_function=len
            )
            docs = text_splitter.split_documents(documents)
            print(f"   âœ“ Split into {len(docs)} chunks")
            
            print("ðŸ’¾ Creating embeddings and saving to database...")
            db = Chroma.from_documents(
                documents=docs,
                embedding=embeddings,
                persist_directory=str(PERSIST_DIRECTORY)
            )
            print(f"âœ… Created Vector Store with {len(docs)} vectors\n")

        return db, embeddings
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        return None, None

# =============================================================================
# RAG CHAIN SETUP
# =============================================================================
def setup_rag_chain(db):
    """Setup RAG chain"""
    print("\n" + "=" * 80)
    print("â›“ï¸ KHá»žI Táº O RAG CHAIN")
    print("=" * 80)
    
    # LLM
    print("\nðŸ¤– [1/3] Connecting to Google Gemini...")
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",
        temperature=0.1,
        max_output_tokens=512,
        convert_system_message_to_human=True,
        request_options={"timeout": 60},
        max_retries=2
    )
    print("   âœ“ Connected to Gemini 2.0 Flash")
    
    # Retriever
    print("ðŸ” [2/3] Creating Retriever...")
    retriever = db.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 2,
            "fetch_k": 5,
            "lambda_mult": 0.7
        }
    )
    print("   âœ“ Retriever ready (top 2 chunks, MMR)")
    
    # Prompt
    print("ðŸ“ [3/3] Creating Prompt Template...")
    template = """Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n má»¹ pháº©m chuyÃªn nghiá»‡p, thÃ¢n thiá»‡n vÃ  hiá»ƒu tÃ¢m lÃ½ khÃ¡ch hÃ ng.

PHÃ‚N LOáº I CÃ‚U Há»ŽI VÃ€ CÃCH TRáº¢ Lá»œI:

ðŸ”¹ **CHÃ€O Há»ŽI/GIAO TIáº¾P CÆ  Báº¢N**
CÃ¢u há»i: "xin chÃ o", "hi", "hello", "chÃ o báº¡n", "hey"
â†’ "ChÃ o báº¡n! ðŸ‘‹ MÃ¬nh lÃ  trá»£ lÃ½ tÆ° váº¥n má»¹ pháº©m. Báº¡n muá»‘n tÃ¬m sáº£n pháº©m gÃ¬ hÃ´m nay? ðŸ˜Š"

ðŸ”¹ **Há»ŽI Vá»€ CHá»¨C NÄ‚NG/GIá»šI THIá»†U**
CÃ¢u há»i: "báº¡n lÃ  ai", "báº¡n lÃ m gÃ¬", "cÃ³ thá»ƒ giÃºp gÃ¬", "báº¡n biáº¿t gÃ¬"
â†’ "MÃ¬nh lÃ  chuyÃªn gia tÆ° váº¥n má»¹ pháº©m! ðŸ’„ MÃ¬nh cÃ³ thá»ƒ giÃºp báº¡n:
â€¢ TÃ¬m sáº£n pháº©m theo loáº¡i da (khÃ´, dáº§u, nháº¡y cáº£m, há»—n há»£p, má»¥n...)
â€¢ TÆ° váº¥n kem dÆ°á»¡ng, serum, toner, máº·t náº¡, sá»¯a rá»­a máº·t, kem chá»‘ng náº¯ng
â€¢ Giáº£i thÃ­ch thÃ nh pháº§n vÃ  cÃ´ng dá»¥ng sáº£n pháº©m
â€¢ Gá»£i Ã½ routine chÄƒm sÃ³c da
Báº¡n Ä‘ang gáº·p váº¥n Ä‘á» gÃ¬ vá» da hoáº·c cáº§n tÃ¬m sáº£n pháº©m nÃ o? ðŸ˜Š"

ðŸ”¹ **Há»ŽI Vá»€ Váº¤N Äá»€ DA**
CÃ¢u há»i: "da tÃ´i bá»‹...", "tÃ´i bá»‹ má»¥n", "da khÃ´", "da dáº§u", "da nháº¡y cáº£m"
â†’ PhÃ¢n tÃ­ch váº¥n Ä‘á» vÃ  Gá»¢I Ã 1-2 sáº£n pháº©m Cá»¤ THá»‚ tá»« database phÃ¹ há»£p nháº¥t

ðŸ”¹ **Há»ŽI THEO LOáº I Sáº¢N PHáº¨M**
CÃ¢u há»i: "cÃ³ kem dÆ°á»¡ng nÃ o...", "serum gÃ¬ tá»‘t", "toner cho da...", "máº·t náº¡..."
â†’ Gá»£i Ã½ 1-2 sáº£n pháº©m PHÃ™ Há»¢P tá»« database, nÃªu rÃµ CÃ”NG Dá»¤NG vÃ  LOáº I DA phÃ¹ há»£p

ðŸ”¹ **Há»ŽI Vá»€ THÆ¯Æ NG HIá»†U**
CÃ¢u há»i: "báº¡n cÃ³ [tÃªn thÆ°Æ¡ng hiá»‡u] khÃ´ng", "sáº£n pháº©m cá»§a [brand]"
â†’ Kiá»ƒm tra database, náº¿u cÃ³ thÃ¬ liá»‡t kÃª, náº¿u khÃ´ng: "MÃ¬nh chÆ°a cÃ³ thÃ´ng tin vá» [brand] trong database. Báº¡n muá»‘n tÆ° váº¥n sáº£n pháº©m theo loáº¡i da hay váº¥n Ä‘á» cá»¥ thá»ƒ khÃ´ng? ðŸ˜Š"

ðŸ”¹ **Há»ŽI SO SÃNH**
CÃ¢u há»i: "A hay B tá»‘t hÆ¡n", "khÃ¡c nhau tháº¿ nÃ o", "nÃªn chá»n cÃ¡i nÃ o"
â†’ So sÃ¡nh 2 sáº£n pháº©m dá»±a trÃªn THÃ€NH PHáº¦N, CÃ”NG Dá»¤NG, LOáº I DA phÃ¹ há»£p

ðŸ”¹ **Há»ŽI GIÃ/MUA á»ž ÄÃ‚U**
CÃ¢u há»i: "giÃ¡ bao nhiÃªu", "mua á»Ÿ Ä‘Ã¢u", "cÃ³ ship khÃ´ng"
â†’ "Xin lá»—i, mÃ¬nh chá»‰ tÆ° váº¥n vá» sáº£n pháº©m thÃ´i nhÃ©. Báº¡n cÃ³ thá»ƒ mua táº¡i cÃ¡c store chÃ­nh hÃ£ng hoáº·c website cá»§a thÆ°Æ¡ng hiá»‡u. MÃ¬nh cÃ³ thá»ƒ tÆ° váº¥n thÃªm vá» sáº£n pháº©m khÃ¡c khÃ´ng? ðŸ˜Š"

ðŸ”¹ **Há»ŽI ROUTINE/CÃCH DÃ™NG**
CÃ¢u há»i: "routine cho da...", "thá»© tá»± dÃ¹ng", "dÃ¹ng nhÆ° tháº¿ nÃ o", "dÃ¹ng khi nÃ o"
â†’ Gá»£i Ã½ routine cÆ¡ báº£n: SÃ¡ng (sá»¯a rá»­a máº·t â†’ toner â†’ serum â†’ kem dÆ°á»¡ng â†’ chá»‘ng náº¯ng), Tá»‘i (tÆ°Æ¡ng tá»± nhÆ°ng thay chá»‘ng náº¯ng = máº·t náº¡ 2-3 láº§n/tuáº§n)

ðŸ”¹ **Cáº¢M Æ N/Táº M BIá»†T**
CÃ¢u há»i: "cáº£m Æ¡n", "thank you", "ok rá»“i", "táº¡m biá»‡t", "bye"
â†’ "KhÃ´ng cÃ³ gÃ¬! ðŸ˜Š ChÃºc báº¡n cÃ³ lÃ n da Ä‘áº¹p! Háº¹n gáº·p láº¡i báº¡n! ðŸ’•"

ðŸ”¹ **CÃ‚U Há»ŽI NGOÃ€I Lá»€**
CÃ¢u há»i: thá»i tiáº¿t, tin tá»©c, thá»ƒ thao, chÃ­nh trá»‹, toÃ¡n há»c...
â†’ "Xin lá»—i, mÃ¬nh chá»‰ chuyÃªn vá» má»¹ pháº©m vÃ  skincare thÃ´i ðŸ’„ Báº¡n cÃ³ muá»‘n há»i vá» chÄƒm sÃ³c da khÃ´ng?"

---

**CHÃš Ã KHI TRáº¢ Lá»œI:**
- LuÃ´n THÃ‚N THIá»†N, dÃ¹ng "mÃ¬nh/báº¡n" thay vÃ¬ "tÃ´i/báº¡n" Ä‘á»ƒ gáº§n gÅ©i hÆ¡n
- Náº¿u tÆ° váº¥n sáº£n pháº©m: Tá»I ÄA 2 sáº£n pháº©m, nÃªu rÃµ TÃŠN - THÆ¯Æ NG HIá»†U - CÃ”NG Dá»¤NG - LOáº I DA
- DÃ¹ng emoji phÃ¹ há»£p: ðŸ˜ŠðŸ’„âœ¨ðŸ’•ðŸ‘‹
- Náº¿u KHÃ”NG cháº¯c cháº¯n: "Báº¡n cÃ³ thá»ƒ mÃ´ táº£ cá»¥ thá»ƒ hÆ¡n vá» [váº¥n Ä‘á»] Ä‘á»ƒ mÃ¬nh tÆ° váº¥n chÃ­nh xÃ¡c hÆ¡n khÃ´ng?"

THÃ”NG TIN Sáº¢N PHáº¨M:
{context}

CÃ‚U Há»ŽI: {question}

TRáº¢ Lá»œI (ngáº¯n gá»n, 2-4 cÃ¢u):"""
    
    prompt = ChatPromptTemplate.from_template(template)
    print("   âœ“ Prompt Template created")
    
    # Build chain
    def format_docs(docs):
        formatted = []
        for i, doc in enumerate(docs, 1):
            content = doc.page_content.strip()
            if content:
                if len(content) > 500:
                    content = content[:500] + "..."
                formatted.append(f"[{i}] {content}")
        return "\n".join(formatted)
    
    rag_chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    
    print("\nâœ… RAG Chain ready!\n")
    return rag_chain

# =============================================================================
# VISION ANALYSIS - STATELESS (accepts PIL Image or base64)
# =============================================================================
def analyze_skin_image(image_input):
    """
    Analyze skin image - STATELESS version
    
    Args:
        image_input: Can be:
            - PIL Image object
            - base64 encoded string
            - file path string (for backward compatibility)
    
    Returns:
        str: Analysis result or None
    """
    try:
        print("\nðŸ“¸ Analyzing skin image...")
        
        # Handle different input types
        if isinstance(image_input, str):
            # Check if it's base64 or file path
            if image_input.startswith('data:image'):
                # Remove data URL prefix
                image_input = image_input.split(',')[1]
            
            # Try to decode as base64
            try:
                image_bytes = base64.b64decode(image_input)
                img = Image.open(io.BytesIO(image_bytes))
            except:
                # Assume it's a file path
                img = Image.open(image_input)
        elif isinstance(image_input, Image.Image):
            img = image_input
        elif isinstance(image_input, bytes):
            img = Image.open(io.BytesIO(image_input))
        else:
            raise ValueError(f"Unsupported image input type: {type(image_input)}")
        
        # Initialize Gemini Vision model
        vision_model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        # Prompt
        vision_prompt = """Báº¡n lÃ  chuyÃªn gia da liá»…u. PhÃ¢n tÃ­ch áº£nh da vÃ  TÃ“M Táº®T NGáº®N Gá»ŒN:

1. LOáº I DA: (khÃ´/dáº§u/há»—n há»£p/nháº¡y cáº£m/thÆ°á»ng)

2. Váº¤N Äá»€ CHÃNH & Má»¨C Äá»˜ NGHIÃŠM TRá»ŒNG:
- Náº¿u cÃ³ má»¥n: loáº¡i má»¥n (viÃªm/Ä‘áº§u Ä‘en/Ä‘áº§u tráº¯ng/bá»c), má»©c Ä‘á»™ (NHáº¸/TRUNG BÃŒNH/Náº¶NG/Ráº¤T Náº¶NG)
- Náº¿u cÃ³ thÃ¢m/sáº¹o: má»©c Ä‘á»™ (NHáº¸/TRUNG BÃŒNH/Náº¶NG/Ráº¤T Náº¶NG), mÃ u sáº¯c, phÃ¢n bá»‘
- Náº¿u cÃ³ lÃ£o hÃ³a: má»©c Ä‘á»™ (NHáº¸/TRUNG BÃŒNH/Náº¶NG)
- Náº¿u cÃ³ váº¥n Ä‘á» khÃ¡c: nÃªu rÃµ

3. Má»¨C Äá»˜ CHUNG: Chá»n 1 trong 4:
   - NHáº¸: Váº¥n Ä‘á» nhá», Ã­t ná»‘t, cÃ³ thá»ƒ tá»± chÄƒm sÃ³c
   - TRUNG BÃŒNH: Váº¥n Ä‘á» rÃµ rÃ ng, nhiá»u ná»‘t, cáº§n sáº£n pháº©m chuyÃªn dá»¥ng
   - Náº¶NG: Váº¥n Ä‘á» lan rá»™ng, viÃªm nhiá»u, cáº§n Ä‘iá»u trá»‹ tÃ­ch cá»±c
   - Ráº¤T Náº¶NG: ViÃªm tráº§m trá»ng, sáº¹o nhiá»u, cáº§n gáº·p bÃ¡c sÄ© da liá»…u

4. Gá»¢I Ã: (1 cÃ¢u ngáº¯n)

QUAN TRá»ŒNG: Pháº£i ghi rÃµ Má»¨C Äá»˜ (NHáº¸/TRUNG BÃŒNH/Náº¶NG/Ráº¤T Náº¶NG).

Tráº£ lá»i NGáº®N Gá»ŒN, báº±ng tiáº¿ng Viá»‡t."""
        
        # Call vision model
        response = vision_model.generate_content([vision_prompt, img])
        analysis = response.text
        
        print("âœ… Analysis complete!")
        return analysis
        
    except Exception as e:
        print(f"âŒ Error analyzing image: {str(e)}")
        return None

# =============================================================================
# HELPER FUNCTIONS FOR NESTJS INTEGRATION
# =============================================================================
def analyze_with_context(question: str, conversation_history: list = None) -> str:
    """
    Analyze question with conversation context (for NestJS)
    
    Args:
        question: User's question
        conversation_history: List of (user_msg, bot_response) tuples
    
    Returns:
        str: AI response
    """
    if conversation_history:
        recent_context = conversation_history[-3:]  # Last 3 exchanges
        context_str = "\n".join([
            f"User Ä‘Ã£ há»i: {ctx[0]}\nBot Ä‘Ã£ tráº£ lá»i: {ctx[1][:200]}..." 
            for ctx in recent_context
        ])
        
        query = f"""Lá»ŠCH Sá»¬ Há»˜I THOáº I Gáº¦N ÄÃ‚Y:
{context_str}

CÃ‚U Há»ŽI HIá»†N Táº I: {question}

HÃ£y tráº£ lá»i dá»±a trÃªn Lá»ŠCH Sá»¬ vÃ  cÃ¢u há»i hiá»‡n táº¡i."""
    else:
        query = question
    
    return query

def build_image_analysis_query(skin_analysis: str, additional_text: str = None, is_severe: bool = False) -> str:
    """
    Build RAG query for image analysis (for NestJS)
    
    Args:
        skin_analysis: VLM analysis result
        additional_text: Optional user text
        is_severe: Whether condition is severe
    
    Returns:
        str: RAG query
    """
    if additional_text:
        return f"""TÃ¬nh tráº¡ng da {'(Ráº¤T NGHIÃŠM TRá»ŒNG - Cáº¦N Gáº¶P BÃC SÄ¨)' if is_severe else ''}:
{skin_analysis}

YÃªu cáº§u: {additional_text}

{'Gá»£i Ã½ 1-2 sáº£n pháº©m Há»– TRá»¢ NHáº¸ NHÃ€NG (khÃ´ng thay tháº¿ Ä‘iá»u trá»‹ y khoa). NHáº¤N Máº NH: Cáº§n gáº·p bÃ¡c sÄ© da liá»…u.' if is_severe else 'TÆ° váº¥n 2-3 sáº£n pháº©m Cá»¤ THá»‚ phÃ¹ há»£p.'}"""
    else:
        return f"""TÃ¬nh tráº¡ng da:
{skin_analysis}

{'Gá»£i Ã½ 1-2 sáº£n pháº©m Há»– TRá»¢. NHáº¤N Máº NH: Cáº§n gáº·p bÃ¡c sÄ©.' if is_severe else 'TÆ° váº¥n 2-3 sáº£n pháº©m phÃ¹ há»£p.'}"""

def check_severity(analysis: str) -> bool:
    """Check if skin condition is severe"""
    return any(keyword in analysis.upper() for keyword in ['Ráº¤T Náº¶NG', 'Ráº¤T NGHIÃŠM TRá»ŒNG'])